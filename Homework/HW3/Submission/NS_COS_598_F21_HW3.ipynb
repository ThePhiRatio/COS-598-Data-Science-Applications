{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02Yewx3TTBH1"
   },
   "source": [
    "# Data Science HW3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D8ZV6LiDHjI"
   },
   "source": [
    "Notebook by: Nicholas C Soucy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiCxLzrrDQz5"
   },
   "source": [
    "For: COS 598 Data Science HW3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAi9ITv0xQws"
   },
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBthw-B5xFch"
   },
   "source": [
    "This cell contains the setting up of pyspark for google colab, all imports, and creation of the spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 477406,
     "status": "ok",
     "timestamp": 1638413476117,
     "user": {
      "displayName": "Nicholas Soucy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiPFW-DnwvujX4meB_5WWNBfdw_89Il0VZt2KSR=s64",
      "userId": "12596739614298951623"
     },
     "user_tz": 300
    },
    "id": "z05WjmvzS67Q",
    "outputId": "4e6b709a-63f1-4dff-9d2b-ce403e1538eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# install java\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "# install spark (change the version number if needed)\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
    "\n",
    "# unzip the spark file to the current folder\n",
    "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
    "\n",
    "# set your spark folder to your system path environment. \n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
    "\n",
    "# install findspark using pip\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5951,
     "status": "ok",
     "timestamp": 1638413482065,
     "user": {
      "displayName": "Nicholas Soucy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiPFW-DnwvujX4meB_5WWNBfdw_89Il0VZt2KSR=s64",
      "userId": "12596739614298951623"
     },
     "user_tz": 300
    },
    "id": "GWCS5svvvRn1"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pyspark.sql.functions as f\n",
    "import re\n",
    "from operator import add\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Google_Page_Rank\").getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWdm_onNTUKB"
   },
   "source": [
    "## Task 1: Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 821,
     "status": "ok",
     "timestamp": 1638413482881,
     "user": {
      "displayName": "Nicholas Soucy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiPFW-DnwvujX4meB_5WWNBfdw_89Il0VZt2KSR=s64",
      "userId": "12596739614298951623"
     },
     "user_tz": 300
    },
    "id": "zcQwd_ztvTjZ",
    "outputId": "f7ef281c-8a72-4294-ac69-6dbe42ec6cb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['A', 'B'], ['B', 'D'], ['C', 'D'], ['D', 'A'], ['A', 'C'], ['B', 'C']]\n"
     ]
    }
   ],
   "source": [
    "#Read data from text file into list\n",
    "data = []\n",
    "file_open = open('drive/MyDrive/Data Science Applications/Homework/HW3/data.txt', \"r\", encoding=\"latin-1\")\n",
    "for line in file_open:\n",
    "    items = line.split(\" \")\n",
    "    data.append([items[0].strip(), items[1].strip()])\n",
    "\n",
    "file_open.close()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7289,
     "status": "ok",
     "timestamp": 1638413490169,
     "user": {
      "displayName": "Nicholas Soucy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiPFW-DnwvujX4meB_5WWNBfdw_89Il0VZt2KSR=s64",
      "userId": "12596739614298951623"
     },
     "user_tz": 300
    },
    "id": "S6FpbSf-15kD",
    "outputId": "c0661ea5-dbdf-466d-fc5b-c96cd747754c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', ['B', 'C']), ('B', ['D', 'C']), ('C', ['D']), ('D', ['A'])]\n"
     ]
    }
   ],
   "source": [
    "#Parse the data into a list where each element is a vertex and all its edges are in a list with it\n",
    "\n",
    "def parseNeighbors(urls):\n",
    "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = re.split(r'\\s+', urls)\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "lines = spark.read.text('drive/MyDrive/Data Science Applications/Homework/HW3/data.txt').rdd.map(lambda r: r[0])\n",
    "\n",
    "# Loads all vertices from input file and initialize their neighbors.\n",
    "edges = lines.map(lambda v: parseNeighbors(v)).distinct().groupByKey().map(lambda x : (x[0], list(x[1])))\n",
    "print(edges.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1638413490381,
     "user": {
      "displayName": "Nicholas Soucy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiPFW-DnwvujX4meB_5WWNBfdw_89Il0VZt2KSR=s64",
      "userId": "12596739614298951623"
     },
     "user_tz": 300
    },
    "id": "kw7fhIlU5QBB",
    "outputId": "6af3ff84-4000-4fe7-8027-e73456efd4d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 1.0), ('B', 1.0), ('C', 1.0), ('D', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "n = edges.count()\n",
    "# initialize ranks to one\n",
    "ranks = edges.map(lambda v: (v[0], 1.0))\n",
    "print(ranks.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14627,
     "status": "ok",
     "timestamp": 1638413505006,
     "user": {
      "displayName": "Nicholas Soucy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiPFW-DnwvujX4meB_5WWNBfdw_89Il0VZt2KSR=s64",
      "userId": "12596739614298951623"
     },
     "user_tz": 300
    },
    "id": "RXb-y9nktnyj",
    "outputId": "8eb93cff-88f3-4a99-ffec-fbead8f57929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 1.0), ('B', 0.5), ('C', 1.0), ('D', 1.5)]\n",
      "[('A', 1.5), ('B', 0.5), ('C', 0.75), ('D', 1.25)]\n",
      "[('A', 1.25), ('B', 0.75), ('C', 1.0), ('D', 1.0)]\n",
      "[('A', 1.0), ('B', 0.625), ('C', 1.0), ('D', 1.375)]\n",
      "[('A', 1.375), ('B', 0.5), ('C', 0.8125), ('D', 1.3125)]\n",
      "[('A', 1.3125), ('B', 0.6875), ('C', 0.9375), ('D', 1.0625)]\n",
      "[('A', 1.0625), ('B', 0.65625), ('C', 1.0), ('D', 1.28125)]\n",
      "[('A', 1.28125), ('B', 0.53125), ('C', 0.859375), ('D', 1.328125)]\n",
      "[('A', 1.328125), ('B', 0.640625), ('C', 0.90625), ('D', 1.125)]\n",
      "[('A', 1.125), ('B', 0.6640625), ('C', 0.984375), ('D', 1.2265625)]\n",
      "[('A', 0.28125), ('B', 0.166015625), ('C', 0.24609375), ('D', 0.306640625)]\n"
     ]
    }
   ],
   "source": [
    "#Page rank algorithm\n",
    "k = 10\n",
    "for i in range(k):\n",
    "  ranks = edges.join(ranks).flatMap(lambda x : [(i, float(x[1][1])/len(x[1][0])) for i in x[1][0]]).reduceByKey(add)\n",
    "  print(ranks.sortByKey().collect())\n",
    "\n",
    "#divide by the number of vertices at end\n",
    "ranks = ranks.map(lambda v: (v[0], v[1]/n))\n",
    "print(ranks.sortByKey().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1039,
     "status": "ok",
     "timestamp": 1638413506035,
     "user": {
      "displayName": "Nicholas Soucy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiPFW-DnwvujX4meB_5WWNBfdw_89Il0VZt2KSR=s64",
      "userId": "12596739614298951623"
     },
     "user_tz": 300
    },
    "id": "sfCqXWsb34H_",
    "outputId": "e0ec5eaf-6afb-40f3-bc04-7ae69ba46a30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A has rank: 0.28125.\n",
      "B has rank: 0.166015625.\n",
      "C has rank: 0.24609375.\n",
      "D has rank: 0.306640625.\n"
     ]
    }
   ],
   "source": [
    "#Print results and save to text file\n",
    "\n",
    "print_list = []\n",
    "for (v, rank) in ranks.sortBy(lambda x: x[0]).collect():\n",
    "    print(\"%s has rank: %s.\" % (v, rank))\n",
    "    print_list.append(str(v) + \" has rank: \" +str(rank) + \".\")\n",
    "\n",
    "textfile = open(\"task_1.txt\", \"w\")\n",
    "for element in print_list:\n",
    "  textfile.write(element + \"\\n\")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wU7y-shFbUJ9"
   },
   "source": [
    "## Task 2: Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkQQX0Hj277I"
   },
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3uZfdUS2-ta"
   },
   "source": [
    "Create a Spark data frame from the RDD containing the page ranks of the vertices 0, 1, 2, and 3 computed in Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2129,
     "status": "ok",
     "timestamp": 1638413508162,
     "user": {
      "displayName": "Nicholas Soucy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiPFW-DnwvujX4meB_5WWNBfdw_89Il0VZt2KSR=s64",
      "userId": "12596739614298951623"
     },
     "user_tz": 300
    },
    "id": "gfBxo2GubTQt",
    "outputId": "7c6fe37d-493e-4e10-ad8c-4a22e7135af5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|Index|       Rank|\n",
      "+-----+-----------+\n",
      "|    0|    0.28125|\n",
      "|    1|0.166015625|\n",
      "|    2| 0.24609375|\n",
      "|    3|0.306640625|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Parallelize and convert list to dataframe\n",
    "db = sqlContext.createDataFrame(ranks.sortByKey(),['Index', 'Rank'])\n",
    "#changing letters to numbers for later task\n",
    "db = db.replace('A', str(0))\n",
    "db = db.replace('B', str(1))\n",
    "db = db.replace('C', str(2))\n",
    "db = db.replace('D', str(3))\n",
    "db.show()\n",
    "\n",
    "db.createOrReplaceTempView('db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8amjla0G4pO4"
   },
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHo9e41o5JIn"
   },
   "source": [
    "Write a Spark SQL query to find the page rank of vertex 2. Print it to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 800,
     "status": "ok",
     "timestamp": 1638413508960,
     "user": {
      "displayName": "Nicholas Soucy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiPFW-DnwvujX4meB_5WWNBfdw_89Il0VZt2KSR=s64",
      "userId": "12596739614298951623"
     },
     "user_tz": 300
    },
    "id": "7qfEiGA-4pV5",
    "outputId": "0bed0f03-927e-4220-ed1c-31fe04d62c37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Index='2', Rank=0.24609375)]\n"
     ]
    }
   ],
   "source": [
    "query = sqlContext.sql(\"SELECT * FROM db WHERE db.Index = 2\")\n",
    "print(query.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OL2jHbPj79yB"
   },
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHXvUEI07-5o"
   },
   "source": [
    "Write a Spark SQL query to find the vertex with the largest page rank. Print both the vertex ID and its page rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 739,
     "status": "ok",
     "timestamp": 1638413509696,
     "user": {
      "displayName": "Nicholas Soucy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiPFW-DnwvujX4meB_5WWNBfdw_89Il0VZt2KSR=s64",
      "userId": "12596739614298951623"
     },
     "user_tz": 300
    },
    "id": "TEDUbsLS8By9",
    "outputId": "836d1c23-e16f-4d11-9dc4-3b75bd4cd15a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Index='3', Rank=0.306640625)]\n"
     ]
    }
   ],
   "source": [
    "query2 = sqlContext.sql(\"SELECT Index, Rank from db ORDER BY Rank DESC LIMIT 1\")\n",
    "print(query2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2627c2EF_Xus"
   },
   "source": [
    "### d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mylSaubH_X3o"
   },
   "source": [
    "Suppose that there is another input text file, where each line contains the meaning of a vertex. Create a Spark data frame from the input text file shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 596,
     "status": "ok",
     "timestamp": 1638413510290,
     "user": {
      "displayName": "Nicholas Soucy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiPFW-DnwvujX4meB_5WWNBfdw_89Il0VZt2KSR=s64",
      "userId": "12596739614298951623"
     },
     "user_tz": 300
    },
    "id": "pjoNi8aH_b3f",
    "outputId": "bed95d8e-022e-43c3-e206-ead928cd493f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Index| Name|\n",
      "+-----+-----+\n",
      "|    0| Adam|\n",
      "|    1| Lisa|\n",
      "|    2| Bert|\n",
      "|    3|Ralph|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read data from text file into list\n",
    "name_data = []\n",
    "file_open = open('drive/MyDrive/Data Science Applications/Homework/HW3/data2.txt', \"r\", encoding=\"latin-1\")\n",
    "for line in file_open:\n",
    "    items = line.split(\" \")\n",
    "    name_data.append([items[0].strip(), str(items[1].strip())])\n",
    "\n",
    "file_open.close()\n",
    "\n",
    "#Create edge rdd from data\n",
    "name_data = sc.parallelize(name_data)\n",
    "\n",
    "names = sqlContext.createDataFrame(name_data,['Index', 'Name'])\n",
    "names.show()\n",
    "names.createOrReplaceTempView('names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oqdj45I5AOc_"
   },
   "source": [
    "### e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ep8GhiU4AOk6"
   },
   "source": [
    "Write a Spark SQL query to join the data frame containing the page rank information and the data frame containing the meaning of each vertex (in this case, the name of the person each vertex corresponds to). Save the query result in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9784,
     "status": "ok",
     "timestamp": 1638413520072,
     "user": {
      "displayName": "Nicholas Soucy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiPFW-DnwvujX4meB_5WWNBfdw_89Il0VZt2KSR=s64",
      "userId": "12596739614298951623"
     },
     "user_tz": 300
    },
    "id": "mhdg93WVAUbP",
    "outputId": "fa7cced2-3020-4869-be34-3261e0e4b21d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Index='0', Name='Adam', Rank=0.28125), Row(Index='1', Name='Lisa', Rank=0.166015625), Row(Index='2', Name='Bert', Rank=0.24609375), Row(Index='3', Name='Ralph', Rank=0.306640625)]\n"
     ]
    }
   ],
   "source": [
    "query3 = sqlContext.sql(\n",
    "    \"\"\"Select names.Index, Name, Rank FROM db, names WHERE db.Index = names.index ORDER BY db.Index\"\"\" )\n",
    "print(query3.collect())\n",
    "query3.write.option(\"header\", True).csv(\"results_2e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4cO3NdcGUDG"
   },
   "source": [
    "## Task 3 BONUS: Wikipedia Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srs1vmCXCwGZ"
   },
   "source": [
    "**NOTE:** Functionally I am confident this would work very well, however, my machine nor goolge colab as enough RAM to simply read the initial text file into a list. Therefore, feel free to run it on a machine that has more RAM as I had no luck. I hope I still get bonus points for my code though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIeiAj6kGZ8F"
   },
   "outputs": [],
   "source": [
    "#Read data from text file into list\n",
    "data = []\n",
    "file_open = open('drive/MyDrive/Data Science Applications/Homework/HW3/enwiki-2013.txt', \"r\", encoding=\"latin-1\")\n",
    "for line in file_open:\n",
    "    items = line.split(\" \")\n",
    "    data.append([items[0].strip(), items[1].strip()])\n",
    "\n",
    "file_open.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ytj_sKi3G_Qc"
   },
   "outputs": [],
   "source": [
    "def parseNeighbors(urls):\n",
    "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = re.split(r'\\s+', urls)\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "lines = spark.read.text('drive/MyDrive/Data Science Applications/Homework/HW3/data.txt').rdd.map(lambda r: r[0])\n",
    "\n",
    "# Loads all vertices from input file and initialize their neighbors.\n",
    "edges = lines.map(lambda v: parseNeighbors(v)).distinct().groupByKey().map(lambda x : (x[0], list(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDoU1Nn0HCoq"
   },
   "outputs": [],
   "source": [
    "n = edges.count()\n",
    "# initialize ranks of them to one\n",
    "ranks = edges.map(lambda v: (v[0], 1.0))\n",
    "print(ranks.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrhYdaXnHqF7"
   },
   "outputs": [],
   "source": [
    "#Page rank algorithm\n",
    "k = 10\n",
    "for i in range(k):\n",
    "  ranks = edges.join(ranks).flatMap(lambda x : [(i, float(x[1][1])/len(x[1][0])) for i in x[1][0]]).reduceByKey(add)\n",
    "  print(ranks.sortByKey().collect())\n",
    "\n",
    "#divide by the number of vertices at end\n",
    "ranks = ranks.map(lambda v: (v[0], v[1]/n))\n",
    "print(ranks.sortByKey().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCphb6qp-yhH"
   },
   "outputs": [],
   "source": [
    "#save results to text file\n",
    "\n",
    "print_list = []\n",
    "for (v, rank) in ranks.sortBy(lambda x: x[0]).collect():\n",
    "    print_list.append(str(v) + \" has rank: \" +str(rank) + \".\")\n",
    "\n",
    "textfile = open(\"task_2.txt\", \"w\")\n",
    "for element in print_list:\n",
    "  textfile.write(element + \"\\n\")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RdDaW7A-9ND"
   },
   "outputs": [],
   "source": [
    "#Parallelize and convert list to dataframe\n",
    "db1 = sqlContext.createDataFrame(ranks.sortByKey(),['Index', 'Rank'])\n",
    "\n",
    "db1.createOrReplaceTempView('db1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5ZcvBxV_FZI"
   },
   "outputs": [],
   "source": [
    "#Read data from text file into list\n",
    "name_data = []\n",
    "file_open = open('drive/MyDrive/Data Science Applications/Homework/HW3/data2.txt', \"r\", encoding=\"latin-1\")\n",
    "for line in file_open:\n",
    "    items = line.split(\" \")\n",
    "    name_data.append([items[0].strip(), str(items[1].strip())])\n",
    "\n",
    "file_open.close()\n",
    "\n",
    "#Create edge rdd from data\n",
    "name_data = sc.parallelize(name_data)\n",
    "\n",
    "names = sqlContext.createDataFrame(name_data,['Index', 'Name'])\n",
    "names.show()\n",
    "names.createOrReplaceTempView('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ej50rCfcIH0X"
   },
   "outputs": [],
   "source": [
    "#add ID --> Names data table for joining\n",
    "db2 = spark.read.option(\"header\",True).csv('drive/MyDrive/Data Science Applications/Homework/HW3/enwiki-2013-names.csv')\n",
    "\n",
    "db2 = db2.withColumnRenamed(\"node_id\",\"Index\")\n",
    "\n",
    "db2.createOrReplaceTempView('db2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IF4xeTUpJrCt"
   },
   "outputs": [],
   "source": [
    "#query to print/save to csv the top 10 wiki pages\n",
    "query3 = sqlContext.sql(\n",
    "    \"Select db2.node_id, name, Rank FROM db1, db2 WHERE db1.node_id = db2.node_id ORDER BY Rank LIMIT 10\")\n",
    "print(query3.collect())\n",
    "query3.write.option(\"header\", True).csv(\"results_3\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMg4GpcXVjT6pSphRok0zqt",
   "collapsed_sections": [],
   "name": "Copy of Copy of NS_COS_598_F21_HW3.ipynb",
   "provenance": [
    {
     "file_id": "1yJMQKuMBJGtUN2ud8AMIh25iKL--Ib0w",
     "timestamp": 1638407007462
    },
    {
     "file_id": "1bV4DX0Ayc16mJx8n2YS_ogSOkXD56ScF",
     "timestamp": 1638305742838
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
